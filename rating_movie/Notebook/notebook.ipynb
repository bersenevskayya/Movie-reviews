{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Влад\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Влад\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords') \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from joblib import load, dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_metrics(history, figure_name):\n",
    "      train=history.history[f'{figure_name}']\n",
    "      val = history.history[f'val_{figure_name}']\n",
    "\n",
    "      epochs = range(1, len(train)+1)\n",
    "      plt.plot(epochs, train, 'b', label = f'Training {figure_name}')\n",
    "      plt.plot(epochs, val, 'b', label = f'Validation {figure_name}', color = 'green')\n",
    "      plt.title(f'Training and Validation {figure_name}')\n",
    "      plt.xlabel('Epochs')\n",
    "      plt.ylabel(f'{figure_name}')\n",
    "      plt.legend()\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test=tf.keras.utils.text_dataset_from_directory(r\"D:\\aclImdb\\test\")\n",
    "train=tf.keras.utils.text_dataset_from_directory(r\"D:\\aclImdb\\train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  rating  label\n",
      "0      Once again Mr. Costner has dragged out a movie...       2      0\n",
      "1      This is an example of why the majority of acti...       4      0\n",
      "2      First of all I hate those moronic rappers, who...       1      0\n",
      "3      Not even the Beatles could write songs everyon...       3      0\n",
      "4      Brass pictures (movies is not a fitting word f...       3      0\n",
      "...                                                  ...     ...    ...\n",
      "24995  I was extraordinarily impressed by this film. ...       8      1\n",
      "24996  Although I'm not a golf fan, I attended a snea...      10      1\n",
      "24997  From the start of \"The Edge Of Love\", the view...       8      1\n",
      "24998  This movie, with all its complexity and subtle...      10      1\n",
      "24999  I've seen this story before but my kids haven'...       7      1\n",
      "\n",
      "[25000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def extract_rating(filename):\n",
    "  parts = filename.split('_')\n",
    "  rating = int(parts[1].split('.')[0])\n",
    "  return rating\n",
    "\n",
    "def create_dataframe(folder_path):\n",
    "  data = []\n",
    "  for i, subfolder in enumerate(os.listdir(folder_path)):\n",
    "    subfolder_path = os.path.join(folder_path, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "      for filename in os.listdir(subfolder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "          filepath = os.path.join(subfolder_path, filename)\n",
    "          with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "          rating = extract_rating(filename)\n",
    "          data.append({'text': text, 'rating': rating, 'label': i})  \n",
    "  return pd.DataFrame(data)\n",
    "\n",
    "folder_path = r\"D:\\aclImdb\\test\"\n",
    "folder_path_2=r\"D:\\aclImdb\\train\"\n",
    "test_df = create_dataframe(folder_path)\n",
    "train_df=create_dataframe(folder_path_2)\n",
    "\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13947</th>\n",
       "      <td>Oh, those sneaky Italians. It's not the first ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22827</th>\n",
       "      <td>I finally got myself set up on mail order DVD ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6261</th>\n",
       "      <td>with a title like this, you know not to expect...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4151</th>\n",
       "      <td>Take \"Rambo,\" mix in some \"Miami Vice,\" slice ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13797</th>\n",
       "      <td>Culled from the real life exploits of Chuck Co...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6196</th>\n",
       "      <td>Some amusing humor, some that falls flat, some...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7386</th>\n",
       "      <td>SPOILERS All too often, Hollywood's Shakespear...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10571</th>\n",
       "      <td>She has been catapulted from 13 to 30, with ma...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6609</th>\n",
       "      <td>Mad Magazine may have a lot of crazy people wo...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22413</th>\n",
       "      <td>The movie \"Holly\" is the story of a young girl...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  rating  label\n",
       "13947  Oh, those sneaky Italians. It's not the first ...       8      1\n",
       "22827  I finally got myself set up on mail order DVD ...       8      1\n",
       "6261   with a title like this, you know not to expect...       3      0\n",
       "4151   Take \"Rambo,\" mix in some \"Miami Vice,\" slice ...       1      0\n",
       "13797  Culled from the real life exploits of Chuck Co...       8      1\n",
       "...                                                  ...     ...    ...\n",
       "6196   Some amusing humor, some that falls flat, some...       4      0\n",
       "7386   SPOILERS All too often, Hollywood's Shakespear...       1      0\n",
       "10571  She has been catapulted from 13 to 30, with ma...       4      0\n",
       "6609   Mad Magazine may have a lot of crazy people wo...       2      0\n",
       "22413  The movie \"Holly\" is the story of a young girl...       7      1\n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.sample(frac=1)\n",
    "test_df = test_df.sample(frac = 1)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(text):\n",
    "      text = text.lower()\n",
    "      text = re.sub('<br />', '', text)\n",
    "      text = re.sub(r\"https\\S+www\\S+https\\S+\", '', text, flags = re.MULTILINE)\n",
    "      text = re.sub(r'\\@w+|\\#', '', text)\n",
    "      text = re.sub(r'^[\\w\\s]', '', text)\n",
    "      text_tokens = word_tokenize(text)\n",
    "      filtered_text = [w for w in text_tokens if not w in stop_words]\n",
    "      return \" \".join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13947    h , sneaky italians . 's first time based movi...\n",
       "22827    finally got set mail order dvd rental could fi...\n",
       "6261     ith title like , know expect great horror movi...\n",
       "4151     ake `` rambo , '' mix `` miami vice , '' slice...\n",
       "13797    ulled real life exploits chuck connors steve b...\n",
       "                               ...                        \n",
       "6196     ome amusing humor , falls flat , decent acting...\n",
       "7386     poilers often , hollywood 's shakespeare adapt...\n",
       "10571    catapulted 13 30 , magic dust involved , court...\n",
       "6609     ad magazine may lot crazy people working ... o...\n",
       "22413    movie `` holly '' story young girl sold poor f...\n",
       "Name: text, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.text = train_df['text'].apply(data_processing)\n",
    "test_df.text = test_df['text'].apply(data_processing)\n",
    "train_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated_count = train_df.duplicated().sum()\n",
    "duplicated_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated_count = test_df.duplicated().sum()\n",
    "duplicated_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop_duplicates('text')\n",
    "test_df = test_df.drop_duplicates('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stemming(data):\n",
    "      text = [stemmer.stem(word) for word in data]\n",
    "      return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Влад\\AppData\\Local\\Temp\\ipykernel_2760\\1322924072.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.text = train_df['text'].apply(lambda x: stemming(x))\n"
     ]
    }
   ],
   "source": [
    "train_df.text = train_df['text'].apply(lambda x: stemming(x))\n",
    "#test_df.text = test_df['text'].apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df['text']\n",
    "X_test = test_df['text']\n",
    "Y_train = train_df['label']\n",
    "Y_test = test_df['label']\n",
    "Z_train = train_df['rating']\n",
    "Z_test = test_df['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "X_train = vect.fit_transform(train_df['text'])\n",
    "X_test = vect.transform(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:25000]\n",
    "X_test =X_test[:25000]\n",
    "Y_train = Y_train[:25000]\n",
    "Y_test = Y_test[:25000]\n",
    "Z_train = Z_train[:25000]\n",
    "Z_test = Z_test[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.toarray()\n",
    "X_test = X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimazer = Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 89ms/step - loss: 14.9511 - mse: 14.9511 - val_loss: 4.9908 - val_mse: 4.9908\n",
      "Epoch 2/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 64ms/step - loss: 3.8466 - mse: 3.8466 - val_loss: 5.2319 - val_mse: 5.2319\n",
      "Epoch 3/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 62ms/step - loss: 2.6704 - mse: 2.6704 - val_loss: 5.3450 - val_mse: 5.3450\n",
      "Epoch 4/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 61ms/step - loss: 2.1421 - mse: 2.1421 - val_loss: 5.2137 - val_mse: 5.2137\n",
      "Epoch 5/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 60ms/step - loss: 1.7735 - mse: 1.7735 - val_loss: 5.2863 - val_mse: 5.2863\n",
      "Epoch 6/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 61ms/step - loss: 1.5426 - mse: 1.5426 - val_loss: 5.4452 - val_mse: 5.4452\n",
      "Epoch 7/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 61ms/step - loss: 1.3623 - mse: 1.3623 - val_loss: 5.2925 - val_mse: 5.2925\n",
      "Epoch 8/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 62ms/step - loss: 1.2673 - mse: 1.2673 - val_loss: 5.4776 - val_mse: 5.4776\n",
      "Epoch 9/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 62ms/step - loss: 1.1475 - mse: 1.1475 - val_loss: 5.2117 - val_mse: 5.2117\n",
      "Epoch 10/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 62ms/step - loss: 1.0384 - mse: 1.0384 - val_loss: 5.4135 - val_mse: 5.4135\n",
      "Epoch 11/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 63ms/step - loss: 0.9845 - mse: 0.9845 - val_loss: 5.2392 - val_mse: 5.2392\n",
      "Epoch 12/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 63ms/step - loss: 0.9201 - mse: 0.9201 - val_loss: 5.2124 - val_mse: 5.2124\n",
      "Epoch 13/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 64ms/step - loss: 0.9298 - mse: 0.9298 - val_loss: 5.3328 - val_mse: 5.3328\n",
      "Epoch 14/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 60ms/step - loss: 0.8548 - mse: 0.8548 - val_loss: 5.0461 - val_mse: 5.0461\n",
      "Epoch 15/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 60ms/step - loss: 0.8088 - mse: 0.8088 - val_loss: 5.1812 - val_mse: 5.1812\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer=optimazer,  loss='mse',  metrics=['mse'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
    "history = model.fit(x = X_train, y = Z_train, epochs = 15, batch_size=16, validation_data= (X_test, Z_test ))\n",
    "check_metrics(history, 'mse')\n",
    "check_metrics(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(model, 'model.joblib')\n",
    "dump(vect, 'tfidf_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 98ms/step - accuracy: 0.6676 - loss: 0.6716 - val_accuracy: 0.8320 - val_loss: 0.5817\n",
      "Epoch 2/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8913 - loss: 0.5233 - val_accuracy: 0.8607 - val_loss: 0.4438\n",
      "Epoch 3/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9179 - loss: 0.3606 - val_accuracy: 0.8791 - val_loss: 0.3482\n",
      "Epoch 4/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9395 - loss: 0.2448 - val_accuracy: 0.8776 - val_loss: 0.3078\n",
      "Epoch 5/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9559 - loss: 0.1703 - val_accuracy: 0.8784 - val_loss: 0.2885\n",
      "Epoch 6/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9722 - loss: 0.1174 - val_accuracy: 0.8777 - val_loss: 0.2912\n",
      "Epoch 7/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9805 - loss: 0.0859 - val_accuracy: 0.8751 - val_loss: 0.3038\n",
      "Epoch 8/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9893 - loss: 0.0566 - val_accuracy: 0.8708 - val_loss: 0.3248\n",
      "Epoch 9/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9918 - loss: 0.0436 - val_accuracy: 0.8695 - val_loss: 0.3401\n",
      "Epoch 10/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9947 - loss: 0.0309 - val_accuracy: 0.8660 - val_loss: 0.3690\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(16, activation='relu'))\n",
    "model2.add(Dense(8, activation='relu'))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(optimizer='rmsprop',  loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "history2 = model2.fit(x = X_train, y = Y_train, epochs = 10, batch_size= 128, validation_data= (X_test, Y_test ))\n",
    "check_metrics(history2, 'accuracy')\n",
    "check_metrics(history2, 'loss')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
